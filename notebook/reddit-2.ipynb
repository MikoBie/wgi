{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "\n",
    "![nlp](../png/nlp.jpg)\n",
    "\n",
    "In this notebook, I will review one of the most fundamental idea in Natural Language Processing (NLP) -- sentiment analysis. We will start with a very naive method and afterward turn to a bit more advanced technique. However, the first glimpse of the sentiment analysis will allow us to understand some basic concepts of natural language processing.\n",
    "\n",
    "## Sentiment analysis\n",
    "\n",
    "One of the most popular techniques in the NLP used extensively in computational social science is **sentiment analysis**. The notion of sentiment refers to the emotional valence of a given text or utterance. Valence is typically defined in this context as a one-dimensional and bipolar construct and refers to the extent to which something is infused with negative or positive emotions/attitudes. However, there are also more elaborate kinds of sentiment analyses that define sentiment in terms of multiple dimensions usually referring to some model of basic (primitive) emotions (i.e. anger, disgust, fear, happiness, sadness, and surprise).\n",
    "\n",
    "Sentiment analysis is often used in analyses of social media, speeches of public persons, press, and many as well as in many other contexts.\n",
    "\n",
    "In technical terms sentiment analysis may be conducted in many different ways (most sophisticated approaches are based on complex deep learning models). Nonetheless, all the methods are always based on some prior linguistic datasets (usually called lexicons or corpora) that assign some scores, representing emotional valences, to give words or phrases.\n",
    "\n",
    "Here I will not bore you with the most basic and naive (and frankly not very useful) sentiment analysis based on the so-called [AFINN](http://corpustext.com/reference/sentiment_afinn.html) lexicon. The lexicon assigns sentiment scores ranging from -5 (very negative) to +5 (very positive) to individual English words (and it includes **only** about 2500 of them). It computes sentiment in a very simple way by just matching individual words from a larger text with their scores in the AFINN lexicon and computing different kinds of average scores based on that.\n",
    "\n",
    "Instead, I will use a more nuanced type of sentiment tailored for web data (such as blog posts, etc.) called [VADER](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109) (see also [GitHub repo](https://github.com/cjhutto/vaderSentiment)), which is implemented in a powerful Python package for NLP called [NLTK](https://www.nltk.org/).\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "One of the biggest difficulties of NLP analysis stems from the fact that natural language is very contextual and messy. Many words may mean different things depending on context or might be spelled or written differently, for instance, depending on their position in a sentence while still being semantically equivalent. Other words may not have any intrinsic meaning as they play only a grammatical role. A good example of that is articles in English (i.e. a, an, the).\n",
    "\n",
    "### Stop words\n",
    "\n",
    "In our simple sentiment analysis, we will be concerned mostly with average sentiment scores over all words in a given text. Therefore, one of our concerns will be to first get rid of words with no clear semantics (which do not convey any message) such as articles. Usually, such words in the context of the NLP are called **stop words**. So we will get rid of all of them from our texts because they would bias our sentiment scores downwards.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "However, first, we have to notice that our approach will be based on the analysis of individual words. And initially, our texts will be single strings. Thus, first, we will have to decompose texts into single words. Such a process is usually called **tokenization** and it refers to a decomposition of a text into lower-order elements such as words or sentences. The naive way to do that would be to split a text by any kind of whitespace, but in practice, this is too simplistic. Luckily, people already studied this problem quite extensively and figured out better solutions, so we will not have to reinvent the wheel. Instead, we will use one of the methods offered by the `NLTK` package.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "After tokenization, we would be able to remove stop words and lookup sentiment scores for the rest of the words. However, there is still some problem that we should address before that. Many words with the same meaning may be written differently in different contexts, for instance, depending on whether they occur in singular or plural form or depending on tense, etc.\n",
    "\n",
    "One way to deal with that is to convert words to their lemmas in the process called **lemmatization**. A lemma of a word is its core form, see the examples below:\n",
    "\n",
    "* houses $\\rightarrow$ house\n",
    "* are $\\rightarrow$ is\n",
    "* mice $\\rightarrow$ mouse\n",
    "* becoming $\\rightarrow$ become\n",
    "\n",
    "In general, the topic of lemmatization is complex, and linguists study this topic, however, for our purposes a very naive approach will be more than enough. Therefore, we will perform a very simple kind of lemmatization that will allow us to simplify all plural forms into singular forms.\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "Summing up, the data processing pipeline that we will use here will be the following:\n",
    "\n",
    "$$\\text{text} \\rightarrow \\text{tokenization} \\rightarrow \\text{lemmatization} \\rightarrow \\text{stop words removal} \\rightarrow \\text{sentiment analysis}$$\n",
    "\n",
    "## Natural Language Toolkit (NLTK)\n",
    "\n",
    "The `NLTK` module is one of the most important and popular Python packages for Natural Language Processing. It is a very powerful but also complex package and we will not discuss any details of how it works. Instead, we will only use a few tools it provides. However, what I will show is enough to conduct a simple sentiment analysis. Thus, the techniques presented here will constitute the last element that together with the things we learned previously will allow you to conduct a simple computational study starting with data extraction and ending with simple natural language analysis.\n",
    "\n",
    "## Read the data\n",
    "\n",
    "Before we move to compute the sentiment we need some text to do so. Therefore, we will use the data you have already collected. We will use comments because usually, they have more text than comments. \n",
    "\n",
    "**But how are we going to do that?**\n",
    "\n",
    "1. Upload the file `comments.jl` to your workspace. To do so you need to click on the small file icon on the left side of the screen. From there you should be able to upload a file with comments.\n",
    "2. Upload `comments.jl` to `python`. It is a similar procedure to the one we used in the previous notebook in the homework.\n",
    "\n",
    "**Note**: Notice that when you open a new notebook from GitHub everything you write out to file disappears from your workspace. Therefore, before you are done with Google Colab download the file you created otherwise it will go forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required module\n",
    "import json\n",
    "\n",
    "## Open the file in read mode\n",
    "with open('comments.jl', 'r') as file:\n",
    "    ## Read line by line and convert every line into a dict\n",
    "    ## Store everything in a list\n",
    "    data = [json.loads(line) for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the illustration of how VADER works, we will need a text which will serve us as an example. It would be appreciated if it was quite long. Therefore, we will just extract from all the comments the longest comment and assign it to the `longest_comment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the empty string\n",
    "longest_comment = \"\"\n",
    "\n",
    "## Loop over the list\n",
    "for line in data:\n",
    "    ## Pick only a string longer than the string before\n",
    "    if len(line['body'])>len(longest_comment):\n",
    "        ## Assign the string to the longest_comment object\n",
    "        longest_comment = line['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment with VADER\n",
    "\n",
    "First, I thought about showing you also a very naive way of computing sentiment - dictionary based but afterward, I realized that there was no point in that. It was enough I told you about it because it is more or less intuitive. `VADER` is a slightly more complex approach that takes into account issues such as exclamation marks, negations, and adjectival modifiers (i.e. words such as \"very\"). Its implementation is much more complex than what we talked about previously and we will not discuss it here. The good thing is that it is implemented in the `NLTK` and is extremely easy to use.\n",
    "\n",
    "As I already mentioned before, one of the characteristic features of NLP is that it is usually based (one way or another) on some preexisting datasets called lexicons and/or corpora compile by linguists and other people who study natural languages. As a result, quite often working with the `NLTK` starts with downloading some additional datasets that will be needed to perform particular analyses. Luckily, this is very easy with `NLTK` as it provides a very simple API for downloading missing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import VADER, download its lexicon, and initialize an instance of a sentiment analyzer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "## Import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Download lexicons\n",
    "nltk.download('vader_lexicon') ## words\n",
    "nltk.download('punkt') ## punctation and signs\n",
    "\n",
    "## Change name of the sentiment function for convinience\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic sentiment analysis is very simple. We need to use the function `vader.polarity_scores` on a given string and it computes four values:\n",
    "\n",
    "* `compound` - is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). \n",
    "* `neu` - the ratio of the proportion of neutral words to all words in a given text.\n",
    "* `neg` - the ratio of the proportion of negative words to all words in a given text.\n",
    "* `pos` - the ratio of the proportion of positive words to all words in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign the results of sentimetn analysis to the object\n",
    "sentiment = vader.polarity_scores(longest_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VADER` allows also computing the sentiment of every single sentence in the given text. To do so you need to use a function `sent_tokenize` which will divide the text into sentences. Obviously, it is not an ideal solution because sometimes it divides a sentence in a strange place but in general, it works very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below we again use list comprehension to create a list of sentiment scores for each sentence in \n",
    "## the text.\n",
    "sent_sentiment = [ vader.polarity_scores(sent) for sent in sent_tokenize(longest_comment) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentiment computed for every single sentence, we might try to see the difference between the average sentiment of each sentence and the sentiment of the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we compute the aveage sentiment of the sentence. Again we use list comprehension and afterward\n",
    "## We sum the resutls and divide by the number of sentences in the text\n",
    "sent_sentiment_compound = sum(s['compound'] for s in sent_sentiment) / len(sent_sentiment)\n",
    "\n",
    "## Print out the resutls\n",
    "print(f\"Average compound score over sentences: {sent_sentiment_compound} while the sentiment of the text is: {sentiment['compound']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the sentiment of all comments we are again going to use a `for-loop`. It will allow us to add to every single mapping in the list fields with sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module with a progress bar\n",
    "import tqdm\n",
    "\n",
    "## It is a simple for-loop and the function tqdm.tqdm prints the progress bar\n",
    "for line in tqdm.tqdm(data):\n",
    "    temp = vader.polarity_scores(line['body'])\n",
    "    line['neg'] = temp['neg']\n",
    "    line['pos'] = temp['pos']\n",
    "    line['neu'] = temp['neu']\n",
    "    line['compound'] = temp['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('faceOff.jl', 'w') as file:\n",
    "    for line in data:\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "This homework will involve more effort than the previous ones because it will require not only collecting something from `Reddit` but also computing something in `R`.\n",
    "\n",
    "1. Collect all comments from under the submission with the following id: `kf9f8i`.\n",
    "2. Compute the sentiment of each comment.\n",
    "3. Write out the results to `faceOff.jl`\n",
    "4. Read the file into `R`.\n",
    "5. Compute a new variable: $correctedSent = compound \\times (1 - neu)$\n",
    "6. Plot the change of the average corrected_sent over time (aggregate the data over a day. **Hint**: use the function `as_date()` from package `lubridate`). \n",
    "7. Send me both `Colab Notebook` and script from `R` (preferably `R Notebook`).\n",
    "\n",
    "**Hint**: In `R` you will need the following packages: `lubridate` - to convert a character variable to date format, `ggplot2` - for plotting (although there are also different ways but I would say with `ggplot2` it would be the easiest), `jsonlite` - to read JSON line file to `R`, `tidyverse` - for data manipulation (you might do it in base `R` but `tidyverse` makes a lot of things easier). The following script will install the packages and load the data to `R`:\n",
    "```R\n",
    "## Check whether the packages we need are already instaleed if not install them\n",
    "packages <- c(\"lubridate\", \"jsonlite\", \"tidyverse\", \"ggplot2\")\n",
    "lapply(packages, function(package){\n",
    "    if (!(package %in% installed.packages())) {\n",
    "        install.packages(package)\n",
    "    }\n",
    "})\n",
    "\n",
    "## Load packages\n",
    "library(lubridate)\n",
    "library(jsonlite)\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "\n",
    "## Read the file to R\n",
    "data <- stream_in(file(\"faceOff.jl\"))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
