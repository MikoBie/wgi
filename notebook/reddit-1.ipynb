{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing more data from Pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important note on accessing data from any kind of API is that the speed of access is always limited. It means that never the speed of accessing the data equals your Internet connection capacity. The limitations set by the server usually are called rate limits. They serve one purpose and one purpose only -- as a defensive measure for service. Shared services (in that case `Pushshift` API) need to protect themselves from excessive use -- whether intended or unintended â€”- to maintain service availability. Even highly scalable systems should have limits on consumption at some level. For the system to perform well, clients must also be designed with rate limiting in mind to reduce the chances of cascading failure. Rate limiting on both the client-side and the server-side is crucial for maximizing throughput and minimizing end-to-end latency across large distributed systems ([source](https://cloud.google.com/solutions/rate-limiting-strategies-techniques)).\n",
    "\n",
    "Normally the best way to learn about rate limits is to ask API about them. However, in terms of `Pushshift`, this seems to be far from an ideal solution. I mean in theory there is an endpoint `meta` which should support you with the knowledge of how many requests per minute you can send. You might find it under this URL: [https://api.pushshift.io/meta](https://api.pushshift.io/meta). However, if you try searching for the answer on Reddit you would learn that what it returns is not always accurate. Therefore, I wouldn't send more than one request per second (in theory it could be 2).\n",
    "\n",
    "Ok, but what would happen if we send too many requests per second? This is quite simple. Instead of getting the status code `200` we would get `429` (or in the case of the `Pushshift` `502` and `523`). This would be the server way of saying that we sent too many requests in the given period and we have to wait ([list of possible status codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)). In some extreme situations, we might get our IP blocked by the server. But it happens only in rare events when we break the rules repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions\n",
    "\n",
    "In practice, it is much easier than it might seem to download more data from `Pushshift`. Especially in terms of submissions. With comments, it is a bit more tricky but also relatively easy. More or less it takes five steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Like always we need to load modules first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load needed modules\n",
    "import requests as rq\n",
    "import json\n",
    "import time\n",
    "import sys ## This is new but we will use it actually for prining only\n",
    "import tqdm ## This is for a progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "This is not something you need to do but I found it easier to define custom functions because repetitively python would execute the same code. As a rule of thumb, you should create a function if you are going to execute the same code more than twice. It is a good practice to either define the custom functions in a separate script or to do it at the beginning of the script. \n",
    "\n",
    "I guess the functions below are self-explanatory. The important notion is to always spend time on writing what the function does. In `python`, you do it typically as I did it because then if you type `help(<name of the fucntion>)` you will get it printed nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define custom functions which will make our life easier\n",
    "def parse_date(date, format='human'):\n",
    "    \"\"\"\"\n",
    "    It takes a string and converts it into either human readable date format or epoch date format\n",
    "    \n",
    "    Parameteres:\n",
    "    ============\n",
    "    date: str\n",
    "        A string with either epoch date format or human readable date format.\n",
    "    format: str\n",
    "        A string defining the format of the input string. By default, it takes the value 'human' and the other option is 'epoch'.\n",
    "    \"\"\"\n",
    "    if format == 'human':\n",
    "        pattern = '%Y-%m-%d %H:%M:%S'\n",
    "        return str(int(time.mktime(time.strptime(date, pattern))))\n",
    "    elif format == 'epoch':\n",
    "        pattern = '%Y-%m-%d %H:%M:%S'\n",
    "        return time.strftime(pattern, time.localtime(int(date)))\n",
    "    \n",
    "def collect_data(source_url, payload):\n",
    "    \"\"\"\n",
    "    It takes the Pushift endpoint and payload as arguments and sends a request to the given URL. Depending on the status code it either returns \n",
    "    the list of mappings, a status code, or sleeps for 60 seconds and tries again to scrape the data.\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    source_url: str\n",
    "        A string with url of the Pushshift endpoint.\n",
    "    payload: \n",
    "        A mapping with parameters passed to the Pushshift API.\n",
    "    \"\"\"\n",
    "    if 'after' not in payload:\n",
    "        payload['after'] = parse_date('2005-06-23 00:00:00')\n",
    "    response = rq.get(source_url, params = payload)\n",
    "    if response.status_code == 200:\n",
    "        time.sleep(1)\n",
    "        return json.loads(response.text)['data']\n",
    "    elif response.status_code == 429 or response.status_code == 523 or response.status_code == 502:\n",
    "        for i in range(60,0,-1):\n",
    "            print(f'\\rThe compulsory break finishes in {str(i)} seconds', end ='', flush=True)\n",
    "            time.sleep(1)\n",
    "        print('\\r' + 100*' ')\n",
    "        return collect_data(source_url = source_url, payload = payload)\n",
    "    else:\n",
    "        return [{'status' : response.status_code, 'message' : response.content }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Define the parameters. Please note that it is a different date format than before. For some reason, you can set the size to be more than 25 even though I read it couldn't be more than that. Anyway, with 100 works so let's keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "payload = { 'subreddit' : 'todayilearned',\n",
    "            'q' : 'science',\n",
    "            'after' : parse_date('2017-10-24 00:00:00'),\n",
    "            'size' : 100}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "I general, probably there is a better way to write this code but I find it the easiest, both for me and I guess for you to understand. This chunk will scrape the first chunk of the data and store it in a data object called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collect_data(source_url = source_url, payload = payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "We didn't talk about `while-loop` but you probably realize by just looking at it that what it does is to run the code inside until the condition is **not** met. So what happens here is that the code is executed until the request to `Pushshift` is empty. I added a few nice features which you might find useful when you run this code for time-consuming search:\n",
    "\n",
    "1. The data is stored after collecting every single batch of data. In our case, it is after approximately 100 submissions. So even though you lose an internet connection or the loop-breaks you will not lose the data you have already collected.\n",
    "2. We don't really know how much data we are going to collect but the progress bar will give you the idea of how much data we have already collected.\n",
    "3. The script will return the status code and also the message whenever something unexpected happens and will stop collecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open a file in write mode\n",
    "with open('submissions.jl', 'w') as file:\n",
    "    ## Write out the data you already collected\n",
    "    for line in data:\n",
    "        line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "    ## Set the prpogress bar\n",
    "    pbar = tqdm.tqdm(position=0, leave=True,initial=100)\n",
    "    ## Create a while-loop\n",
    "    while len(data) > 0:\n",
    "        ## Check if we got data from Reddit or a strange status code\n",
    "        if len(data[0].keys()) > 2:\n",
    "            ## Get the last collected data date in epoch time\n",
    "            after = parse_date(data[-1]['created_utc'])\n",
    "            ## Update the payload after field\n",
    "            payload['after'] = after\n",
    "            ## Collect the data\n",
    "            data = collect_data(source_url = source_url, payload = payload)\n",
    "            ## Write out the collected data to the file\n",
    "            for line in data:\n",
    "                if 'created_utc' in line:\n",
    "                    line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "                    file.write(json.dumps(line) + '\\n')\n",
    "            ## Update the progress bar\n",
    "            pbar.update(len(data))\n",
    "        else:\n",
    "            ## Print out the strange status code and its message\n",
    "            print(f'Something went wrong. The status code error was {data.pop}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "It is a bit more tricky to access the comments than submissions but at the end of the day not that hard. First, you need to decide what you would want to access. You have multiple options:\n",
    "\n",
    "* get all comments from the specific subreddit - it is relatively the easiest, however, it will take days to collect all comments or at least hours for big subreddits. You need to specify the `subreddit` field and nothing else (however, it is wise to change the default number of collected items to the maximum - `'size' : 100`). Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'subreddit' : 'todayilearned',\n",
    "            'size' : 100 }\n",
    "```\n",
    "* get all comments from under the specific submission. You need to specify the `link_id` field and nothing else (however, it is wise to change the default number of collected items to the maximum - `'size' : 100`). The important note is that as a `link_id` you should put an `id` of the submission. Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'link_id' : '8vjr2l',\n",
    "            'size' : 100 }\n",
    "```\n",
    "* get all comments from under the comment. You need to specify the `parent_id` field and nothing else (however, it is wise to change the defualt number of collected items to the maximum - `'size' : 100`). The important note is that as a `parent_id` you should put an `id` of the comment. Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'parent_id' : 'e1nuxpc',\n",
    "            'size' : 100 }\n",
    "```\n",
    "Obviously, you can specify also other fields to narrow the search, however, I guess for our purposes it will not be necessary.\n",
    "\n",
    "Actually, the procedure of scraping the comments is as in terms of submissions. The only difference is the `payload` and `source_url`. However, we do not have to repeat in this particular notebook the first two steps cause you need to do it only once in the Notebook likewise in `R`. Therefore, the below code consists of steps from 3 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "payload = { 'link_id' : '8vjr2l',\n",
    "            'size' : 100 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comments = collect_data(source_url = source_url, payload = payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open a file in write mode\n",
    "with open('comments.jl', 'w') as file:\n",
    "    ## Write out the data you already collected\n",
    "    for line in data_comments:\n",
    "        line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "    ## Set the prpogress bar\n",
    "    pbar = tqdm.tqdm(position=0, leave=True,initial=100)\n",
    "    ## Create a while-loop\n",
    "    while len(data_comments) > 0:\n",
    "        ## Check if we got data from Reddit or a strange status code\n",
    "        if len(data_comments[0].keys()) > 2:\n",
    "            ## Get the last collected data date in epoch time\n",
    "            after = parse_date(data_comments[-1]['created_utc'])\n",
    "            ## Update the payload after field\n",
    "            payload['after'] = after\n",
    "            ## Collect the data\n",
    "            data_comments = collect_data(source_url = source_url, payload = payload)\n",
    "            ## Write out the collected data to the file\n",
    "            for line in data_comments:\n",
    "                if 'created_utc' in line:\n",
    "                    line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "                    file.write(json.dumps(line) + '\\n')\n",
    "            ## Update the progress bar\n",
    "            pbar.update(len(data_comments))\n",
    "        else:\n",
    "            ## Print out the strange status code and its message\n",
    "            print(f'Something went wrong. The status code error was {data_comments.pop}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "I know there is not much time left for Tuesday but it is a relatively easy task. Please, pick a subreddit of your choice. Collect all the submissions which have been posted since Klay Thompson tore his right Achilles. Afterward, pick the one with the most comments and download all of them.\n",
    "\n",
    "**Hint**: To read data from the file to python you simply need to execute this code:\n",
    "```python\n",
    "with open('submissions.jl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "```\n",
    "The code above might look complicated but it is not that complex. The first line opens the file in the read mode. The second is a shorter way of creating a list in a `for-loop`. What it says is just:\n",
    "1. `file.readlines()` - read the file line by line.\n",
    "2. `for line in file.readlines()` - in a loop create a temporary object `line` and store there the line you just read from the file.\n",
    "3. `json.loads(line)` - convert the line read from the file to a mapping object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read submissions\n",
    "with open('submissions.jl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "    \n",
    "## Select submission with the biggest number of comments\n",
    "comments_num = 0\n",
    "submission_id = ''\n",
    "for line in data:\n",
    "    if line['num_comments'] > comments_num:\n",
    "        comments_num = line['num_comments']\n",
    "        submission_id = line['id']\n",
    "\n",
    "## Define url and payload\n",
    "source_url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "payload = { 'link_id' : submission_id,\n",
    "            'size' : 100}\n",
    "\n",
    "## Collect first batch of data\n",
    "data_comments = collect_data(source_url = source_url, payload = payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open a file in write mode\n",
    "with open('comments.jl', 'w') as file:\n",
    "    ## Write out the data you already collected\n",
    "    for line in data_comments:\n",
    "        line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "    ## Set the prpogress bar\n",
    "    pbar = tqdm.tqdm(position=0, leave=True,initial=100)\n",
    "    ## Create a while-loop\n",
    "    while len(data_comments) > 0:\n",
    "        ## Check if we got data from Reddit or a strange status code\n",
    "        if len(data_comments[0].keys()) > 2:\n",
    "            ## Get the last collected data date in epoch time\n",
    "            after = parse_date(data_comments[-1]['created_utc'])\n",
    "            ## Update the payload after field\n",
    "            payload['after'] = after\n",
    "            ## Collect the data\n",
    "            data_comments = collect_data(source_url = source_url, payload = payload)\n",
    "            ## Write out the collected data to the file\n",
    "            for line in data_comments:\n",
    "                if 'created_utc' in line:\n",
    "                    line['created_utc'] = parse_date(line['created_utc'], format = 'epoch')\n",
    "                    file.write(json.dumps(line) + '\\n')\n",
    "            ## Update the progress bar\n",
    "            pbar.update(len(data_comments))\n",
    "        else:\n",
    "            ## Print out the strange status code and its message\n",
    "            print(f'Something went wrong. The status code error was {data_comments.pop}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
