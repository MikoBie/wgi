{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"../png/reddit.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reddit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, I would say it is a good practice to start with learning what Reddit is. Below I copied some basics information from their [help page](https://www.reddithelp.com/hc/en-us/articles/204511479-What-is-Reddit/). They give the following answer to the question of what Reddit is:\n",
    "> * Reddit is a source for what's new and popular on the Internet.\n",
    "> * Users like you provide all of the content and decide, through voting, what's good and what's junk.\n",
    "> * Reddit is made up of many individual communities, also known as subreddits. Each community has its own page, subject matter, users, and moderators.\n",
    "> * Users post stories, links, and media to these communities, and other users vote and comment on the posts.\n",
    "> * Through voting, users determine what posts rise to the top of community pages and, by extension, the public home page of the site.\n",
    "> * Links that receive community approval bubble up towards #1, so the front page is constantly in motion and (hopefully) filled with fresh, interesting links.\n",
    "\n",
    "Personally, I *do not have* an account on Reddit and probably not planning to have one, but if you want to understand better what kind of data you can extract from there I would recommend setting an account. As far as I understand Reddit is a big old internet forum (similar to 4chan or Polish Wykop) in which users post or comment on different information. Actually, every user can perform four types of actions:\n",
    "\n",
    "1. Create a subreddit. Basically, it is a subforum on a given topic in which a group of users discusses it.\n",
    "2. Write a post (submission) in a given subreddit.\n",
    "3. Write a comment to a given post.\n",
    "4. Rate a given comment or post.\n",
    "\n",
    "For these actions, people earn **karma**.\n",
    "\n",
    "### What is karma?\n",
    "\n",
    "Again, according to [Reddit's help page](https://www.reddithelp.com/hc/en-us/articles/204511829-What-is-karma-) karma is:\n",
    "\n",
    ">A user's **karma** reflects how much a user has contributed to the Reddit community by an approximate indication of the total votes a user has earned on their submissions (\"post karma\") and comments (\"comment karma\"). When posts or comments get upvoted, that user gains some karma. You can see how much karma a user has on their profile page.\n",
    ">\n",
    ">Karma is only approximate: there is not a 1:1 relationship with votes. Your post karma will always be significantly lower than the total number of votes you receive on your links. Comment karma is closer to a 1:1 relationship but is still only approximate.\n",
    "\n",
    "Therefore, there are two important pieces of information here. First, users differ in karma points which are based on their activity and the popularity of the content they created. This information might be useful when/if we learn how to get information on users. Second, posts (submissions) or comments might be either upvoted or downvoted. This is important because as far as I understand the comments or posts (submissions) with the highest score are exposed on the front page of Reddit and might have a bigger impact on the users not necessarily only the given subreddit. Also, comments with a high score are displayed higher under the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![api](../png/api.jpg)\n",
    "\n",
    "## What is API\n",
    "When we know something about Reddit let's dig a bit deeper into the world of ~restaurants~ APIs.\n",
    "In general, web APIs (Application Programming Interface) are publicly (usually; there is plenty of private APIs, but for obvious reasons, we do not care about them as we can not use them) available interfaces through which third parties (this is us!) can access some data resources in a **remote**, **reliable** and **programmable** manner.\n",
    "\n",
    "What does it mean in practice?\n",
    "\n",
    "* **Remote.** Users can access the resource from anywhere, provided they have an internet connection.\n",
    "* **Reliable.** The interface exposed to users is independent of the internal details of the system that produces the data. In other words, the way a user communicates with the API is independent of the way the system works. In practice it means that a user does not have to know anything about the system, it is enough to know the API interface.\n",
    "* **Programmable.** API can be interacted with based on a predefined set of commands/methods (an interface) in a way that can be expressed with a programming language. This is usually achieved by using HTTP protocol which is a standard communication protocol in the Web and for which utilities are available in any major programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API\n",
    "In general, we now should know what API is and what Reddit is. So it is the right time to [talk about practice](https://www.youtube.com/watch?v=eGDBR2L5kzI), i.e. where to find Reddit's API. This question is more complex than it might seem. There are two ways to access Reddit's data through the API:\n",
    "\n",
    "1. **Official Reddit API.** In most cases the best way to access data from a webpage (social media platform) that has an API is to use the official one. You might find documentation on Reddit's [here](https://www.reddit.com/dev/api). This webpage is not particularly beautiful but rarely documentation is. At first glance, you probably would be overwhelmed with the amount of information you find there. However, for now, you only need to know that you are not going to use the official Reddit API cause it is inconvenient. It requires authentication (having a developer account) and as far as I am concerned it is not really developed. Anyhow, if you decide to perform a more detailed analysis of Reddit you probably should read the official documentation and visit these two pages: [Reddit's Archived GitHub repository](https://github.com/reddit-archive/reddit/wiki/API) and [Documentation on Reddit's API Python Wrapper](https://praw.readthedocs.io/en/latest/). This is a lot of reading and understanding, however, there is no other way unless...\n",
    "2. **Pushshift Reddit's API.** There is a Reddit user Jason Baumgartner who for unclear reasons (at least for me they are unclear but I was not particularly motivated to look it up) decided to dump the whole Reddit. Moreover, he created an API to access the data he collects. On [this](https://pushshift.io) much nicer webpage you will find documentation on his API.\n",
    "\n",
    "In our case, we will use **Pushshift Reddit API**. It is much easier to use and for our purposes it will be more than enough. As far as I know, it does not allow to collect exactly the same data as when using the **Official Reddit's API**, however, it has a huge advantage of not requesting authentication. When we are using Pushshift we need to remember a few things:\n",
    "\n",
    "1. It is less reliable than the official API because it is run by a single person (half-truth).\n",
    "2. It does not offer the same functionality as the official API.\n",
    "3. It is likely to introduce some kind of authentication in the future.\n",
    "4. It streams the data live from Reddit hence post's scores are not that reliable.\n",
    "\n",
    "## API and where to find it?\n",
    "\n",
    "So in simple terms, an API is an interface using which you send a specific message (request) and get something back (response). In the case of **Pushshift**, it lives under the following [url](https://api.pushshift.io). However, if you click on it a blank page will open. For some reason, it works like this but the more common practice is to use the API address to put the documentation there ([Wikipedia](https://en.wikipedia.org/w/api.php) does exactly that). You can find documentation of Pushshift API [here](https://pushshift.io/api-parameters/). Before you move any further you should start reading it. Why? Because you need to know what the API can offer you. In other words what kind of data you might access.\n",
    "\n",
    "Using Pushshift API you might access either submissions or comments even though in the [docummantation](https://pushshift.io/api-parameters/) they state something different. Therefore, it is better to visit their [GitHub repository](https://github.com/pushshift/api). To access submissions or comments we will use something which is called endpoints. If an API is an interface endpoint is a communication channel. In Pushshift there are two (if you click on any of the links you should see the last 25 comments or last 25 submissions):\n",
    "\n",
    "* [https://api.pushshift.io/reddit/submission/search](https://api.pushshift.io/reddit/submission/search)\n",
    "* [https://api.pushshift.io/reddit/comment/search](https://api.pushshift.io/reddit/comment/search)\n",
    "\n",
    "Before I will tell you how to look for specific comments let's focus for a second on what you have just seen under either of these two links. The displayed data was in a bit peculiar format. You might have seen it before but probably you don't know how to use it. It is called JSON and it is a bit different than the traditional tabular data format. I will explain it in the next section.\n",
    "\n",
    "## What is JSON?\n",
    "\n",
    "Imagine that you are meant to somehow extract the most important information from the following text and write it up in the database:\n",
    "\n",
    ">Alice is a *17* years old young lady. Although her main field of interest is physics (especially quantum physics and string theory), she also fancies sport. Her favorite physical activities are fishing and football. Bob, on the other hand, is a naughty 15 years old boy who only loves literature, especially Szymborska poems touches his heart.\n",
    "\n",
    "So one way of doing it would be to put in the table like this:\n",
    "\n",
    "|Name | Sex | Age | Interest A | Interest A1 | Interest A2 | Interest B | Interest B1 | Interest B2|\n",
    "|-----|-----|-----|------------|-------------|-------------|------------|-------------|------------|\n",
    "Alice | F   | 17 | physics | quantum physics | string theory | sport | fishing | football|\n",
    "Bob | M | 15 | literature | poems | n/a | n/a | n/a | n/a |\n",
    "\n",
    "However, this would not be informative and also we would lose some space for empty columns in the second row. On the bigger than two records scale we would like to avoid wasting resources for empty cells. Therefore, smart people invented a better way to store this kind of data - JSON. The same data as above we can store in it in the following manner:\n",
    "```json\n",
    "{ \"name\" : \"Alice\",\n",
    "  \"Sex\" : \"F\",\n",
    "  \"Age\" : 17,\n",
    "  \"intersts\" : [ { \"name\" : \"physics\", \"fields\" : [ \"quantum physics\", \"string theory\" ] },\n",
    "                 { \"name\" : \"sport\", \"fields\" : [ \"football\", \"fishing\" ] } ]\n",
    "}\n",
    "{ \"name\" : \"Bob\",\n",
    "  \"Sex\" : \"M\",\n",
    "  \"Age\" : 15,\n",
    "  \"interests\" : { \"name\" : \"literature\", \"fileds\" : \"poems\" }\n",
    "}\n",
    "```\n",
    "Usually, this is the format that we will get from API. I mean there is a possibility to get data in XML format but then everything gets quite complicated. In most cases, you don't have to worry cause it will be JSON. This is good information cause Python has a special object called a dictionary which is really similar to JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So enough talking, let's see this API-thing in a real life. I am not going to talk too much about details in terms of _python_ syntax or modules. If you are curious you might either visit this great [resource](https://www.learnpython.org) or use one of [my notebooks](https://github.com/sztal/ecss-class). Either way, it is worth learning some basics of python. Instead of teaching you python, here, I will try to show you a basic code that will allow you to access data from Reddit, write it out to a JSON file, and load it to _R_.\n",
    "\n",
    "### Step 1.\n",
    "First things first. Likewise in _R_, we will start with loading libraries. In terms of _python_, they are called modules. Below I load three modules that we will use in this script. Because we are using Google Colab we do not have to install them. If you were using _python_ on your personal computer you would have to install them first. It is the same in _R_, where you have to install the package only once, and afterward, you might use it till the world's end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq ## this is a module to send requests\n",
    "import json ## this is a module to process json\n",
    "import time ## this is a module we will need to understand time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.\n",
    "Let's define our two endpoints as `strings`. It makes everything more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comments endpoint \n",
    "url_comments = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "## Submissions endpoint\n",
    "url_submissions = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.\n",
    "When we clicked the link of each endpoint we got a random 25 comments or submissions, but when we looked at the documentation we saw that there were plenty of different parameters we could specify. I would recommend visiting this website where the [documentation](https://github.com/pushshift/api) is presented more compactly and understandably. Below I only use two parameters but you might want to specify. However, you need to be careful with after and before because they are in what might seem like a strange format. You might use two functions from the `time` module: \n",
    "\n",
    "* `time.strptime()` is a function to convert a string into a date format based on a specific pattern. In other words, we tell _python_ that `'30.08.2011 11:05:02'` is not a normal string but a date object of the following pattern `'%d.%m.%Y %H:%M:%S'`;\n",
    "* `time.mktime()` is a function to convert a normal date into epoch format, for exmaple it converts '`30.08.2011 11:05:02`' into the following number `1314695102`.\n",
    "\n",
    "The usage of both of them would look something like this:\n",
    "```python\n",
    "date_time = '30.08.2011 11:05:02'\n",
    "pattern = '%d.%m.%Y %H:%M:%S'\n",
    "after_time = str(int(time.mktime(time.strptime(date_time, pattern))))\n",
    "```\n",
    "At the first glance, it might look a bit complex but when you think about it it is not. I mean I do not expect you to know it but to understand it on a very general level. The only thing you will have to change below data_time to the dates of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = '30.08.2011 11:05:02'\n",
    "pattern = '%d.%m.%Y %H:%M:%S'\n",
    "after_time = str(int(time.mktime(time.strptime(date_time, pattern))))\n",
    "\n",
    "payload = { 'subreddit' : 'climate',\n",
    "            'after' : after_time }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.\n",
    "So when we know the URL of the endpoint and options we want to pass we should send the request to this URL. We will use the `get` function from the `requests` module. However, unlike in R, we need to tell python from which module that function is. Therefore, we will use `rq.get()`.\n",
    "This function will take as the first argument the endpoint URL and as the second argument specific options, we want to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's send the request and save the response as the response object\n",
    "response = rq.get(url_submissions, params = payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just execute the chunk above we will get only a mysterious and enigmatic code 200. This is good information. It means that we got a valid response from the server. There are multiple different codes we could get when we send the request to the server but you should be aware of two: [5xx](https://github.com/500) and [4xx](https://www.pixar.com/404). In general, the former means that there is an issue on the server-side and the latter that the resource you are looking for [does not exist](https://github.com/404)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.\n",
    "Ok, but how to extract from this response object some data? It is easier than it might look like but what we need to do is to use a method text on the object. I am not going too much into details but in _python_ objects might have methods (functions) which might be run of them. It is a bit like the internal ability of the object that is executed on the object in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now our `response.text` object for _python_ is just a not really interesting long string. To be able to process it further (mainly save it into a data file) we need to transform it into a _python_ representation of JSON. We will do it by using function `json.loads()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you look closely at the object above you will realize it is a single curly bracket object. It has one key - data and one value which is a list. You can either believe me or we can just check it in _python_ by using method `keys()` on the `json.loads(response.text)` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response.text).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract and save the value from this key we will execute the following code which is a bit similar to what you are used to in _R_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6. \n",
    "Let's save the file now to the JSON line file on our computer. It is not that complicated as it might seem. We are simply opening a file and then writing every single line from our data object into this file. The details are not that important so I will not go over every single line of the code. When saving the file I convert this strange date format to a more useful one: `'Y-m-d H:M:S'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('climate.jl', 'w') as file:\n",
    "    for line in data:\n",
    "        line['created_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['created_utc']))\n",
    "        if 'author_created_utc' in line.keys():\n",
    "            line['author_created_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['author_created_utc']))\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.\n",
    "This is specific to only Google Colab and it will just download the file `climate.jl` to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The code below should download the file from Google Colab but for some reason it does not work\n",
    "## from google.colab import files\n",
    "## files.download('climate.jl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
